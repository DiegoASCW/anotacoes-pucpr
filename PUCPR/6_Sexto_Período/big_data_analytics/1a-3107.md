python - biblioteca: spark
mysql

big data: definição do Gartener + NIST: Alto Volume, Alta Velocidade, em Alta Variedade, onde as técnicas tradicionais não surtem efeito.
No entanto, quando o volume é tão alto, precisamos certificar que o dado está certo (coletado e formatado e processado certo)

Tipos de dados:
- Estruturado: campos bem definidos ao ponto de poder assumir a forma como ele vêm 
- Semi-Estruturado: quando existe estrutura definida previamente, porém não estão organizados(quando o formato muda de acordo com o contexto)
- Não estruturado: sem nenhuma estrutura definida previamente

Para big data, a infraestrutura deve ser diferente, onde seus desafios são:
- Falta de escalabilidade, processamento lento
- Busca em disco (disco é muito lento o que gera gargalo)
- Falta de confiabilidade
	- 1000 máquinas processando, 1000 máquinas podem falhar

HDFS: soluciona o problema de armazenar um arquivo muito grande, onde este é quebrado em blocos e distruído para vários outros blocos.
- arq. Mestre-Escravo: uma máquina coordena para quem irá armazenar tal bloco1 e quem irá bloco2, ..., mantendo um metadado sobre onde foi cada bloco e sua ordem (NameNode)
- Para garantir tolerância a falhas, precisamos de replicação dos dados, não tem como ser diferente, nem tenta
- E se o NameNode falhar, usa-se o Sec.NameNode

MapReduce: framework para filtro, ordenação, e agrupamento de dados distribuídos.
- Utiliza do mesmo conceito de Mestre-Escravo, onde o mestre é chamado de "JobTracker", e cada escravo "TaskTracer" (o MapReducer roda em cada DataNode)
- Para saber qual máquina irá agrupar os dados, é feito um hash do filtro realizado localmente, onde é feito uma divisão pelo número de máquinas presentes no ambiente, e o resultado da conta é o index da máquina onde será agrupado os dados

>Quais entradas eu filtro
>Quais campos eu filtro
>Qual a chave
>Qual valor